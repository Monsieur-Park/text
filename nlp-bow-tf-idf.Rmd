---
layout: page
title: "xwMOOC 자연어 처리 - 텍스트"
subtitle: "BoW와 TF-IDF"
author:
    name: xwMOOC
    url: https://www.facebook.com/groups/tidyverse/
    affiliation: Tidyverse Korea
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
    toc_float: true
    highlight: tango
    code_folding: show
    number_section: true
    self_contained: true
editor_options: 
  chunk_output_type: console
---

``` {r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
                    comment="", digits = 3, tidy = FALSE, prompt = FALSE, fig.align = 'center')
```

# 텍스트 데이터 준비 {#tidytext-dataset}

먼저 `crude` 텍스트 데이터, 즉 영문 뉴스기사를 `tidytext` 데이터로 준비한다.

```{r tidytext-dataset}
library(tidyverse)
library(tm)
library(tidytext)

data("crude")

crude_tbl <- tidy(crude)
crude_text <- crude_tbl %>% select(id= oldid, text)
crude_text %>% 
  slice(1)
```

# 단어주머니 (BoW, Bag of Word)와 TF-IDF {#tidytext-dataset-bow}

## 토큰화 {#tidytext-dataset-bow-token}

먼저 석유관련 뉴스기사를 단어(`words`) 기준으로 토큰화시킨다.
그리고 나서 `anti_join()` 함수로 영어의 특성인 불용어를 제거한다.

```{r crude-token}
crude_text %>% 
  unnest_tokens(output="word", token = "words", input=text) %>% 
  anti_join(stop_words)
```

## 단어주머니 &rarr; TF-IDF {#tidytext-dataset-bow-token-bow}

두번째로 단어별 빈도수를 단어주머니 기법으로 계량화시킨다.
문서별 빈도수를 `count()` 함수로 계산하고, `bind_tf_idf()` 함수로 TF-IDF를 계산한다.

```{r crude-token-bow}
(crude_tfidf <- crude_text %>% 
  unnest_tokens(output="word", token = "words", input=text) %>% 
  anti_join(stop_words) %>% 
  count(id, word, sort=TRUE) %>% 
  bind_tf_idf(word, id, n))
```


# 코사인 유사도(Cosine Similarity) {#tidytext-dataset-token-bow-cosine}

`widyr` 팩키지 `pairwise_similarity()` 함수를 사용해서 뉴스간 유사도를 측정할 수 있다.

```{r crude-token-bow-cosine-similairty}
crude_tfidf %>% 
  widyr::pairwise_similarity(id, word, tf_idf) %>% 
  arrange(desc(similarity))
```

가장 유사도가 높은 것으로 나온 뉴스 두개를 살펴보자.

```{r crude-token-bow-cosine-news}
crude_text %>% 
  filter(id %in% c(12672, 12685))
```