---
layout: page
title: "xwMOOC 텍스트"
subtitle: "텍스트 분류(Text Classification) - 나이브 베이즈(naive bayes)"
author:
    name: xwMOOC
    url: https://www.facebook.com/groups/tidyverse/
    affiliation: Tidyverse Korea
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
    toc_float: true
    highlight: tango
    code_folding: show
    number_section: true
    self_contained: true
editor_options: 
  chunk_output_type: console
---

``` {r, include=FALSE}
source("tools/chunk-options.R")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)

options(scipen = 999)
options(dplyr.width = 120)
options(dplyr.print_max = 1e9)
```

# 텍스트 분류(Text classification) [^apac-machine-learning-summit]

[^apac-machine-learning-summit]: [송치성, "Naïve or not", 바벨피쉬, APAC 머신러닝, 데이터 사이언스 커뮤니티 서밋](http://onoffmix.com/event/97444)

분류기는 사전 분류될 집단이 정해진 경우 어떤 집단에 속할 것인지 판별하여 지정하는 역할 수행한다.
텍스트 분류의 종류는 다음과 같은 것이 있다.

- 주제(Topic): 텍스트가 무엇에 관한 것인가 분류.
- 감정(Sentiment): 다양한 감정 상태 중 어떤 감정 상태에 가까운가 분류.
- 언어(Language): 어느 나라, 어느 민족 텍스트인지 분류.
- 쟝르(Genre): 텍스트가 어떤 쟝르(시, 소설, 등)에 속하는지 분류.
- 저자(Author): 텍스트를 보고 누구의 글인지 판별.
- ...

# 베이즈 정리를 활용한 텍스트 분류기

전자우편/채팅이 스팸일 확률을 찾고자 하는데 베이즈 정리로 출발해서, 
단어가 하나인 경우를 경유하여 단어를 다수 포함한 전자우편/채팅으로 확장해 보자.

## 베이즈 정리 

특정 단어가 나오면 그 전자우편/채팅이 스팸일 확률을 베이즈 정리를 활용하면 다음과 같다.

$$ P(s|w) = \frac {P(s,w)}{P(w)} = \frac {P(w|s)P(s)}{P(w)} $$

여기서, $P(w):$ 전자우편이나 채팅에서 특정단어가 출현할 확률(예를 들어, Sell, 팔아요 등), $P(s)$는 전자우편이나 채팅이 스팸일 확률

$$P(w) = P(w, s) + P(w, \sim s)$$

## 단어 한개를 가정한 베이즈 정리

상기 관계를 이용하여 단어가 주어졌을 때 스팸일 확률을 다음과 같이 정리할 수 있다.

$$ P(s|w) = \frac {P(w|s)P(s)}{P(w)} = \frac {P(w|s)P(s)}{P(w, s) + P(w, \sim s)} = \frac {P(w|s)P(s)}{P(w|s)P(s) + P(w| \sim s)P(\sim s)}$$

즉, 스팸중에서 특정 단어가 출현할 조건부 확률 $P(w|s)$ 과 스팸이 아닌 것 중에 특정 단어가 출현할 확률 $P(w|\sim s)$만 알게 된다면 
단어가 주어졌을 때 스팸일 확률을 계산할 수 있게 된다.

## 다수 단어를 가정한 베이즈 정리

우선 다수 단어를 $W$라고 가정한다. 즉, $W = w_1 , w_2 , \cdots , w_n$ 라고 다수 단어를 표현한다.
그리고, 각 단어가 서로 독립적이라는 가정을 넣으면 수식은 다음과 같이 단순화된다.

$$\begin{aligned} 
   P(s|W) &= \frac {P(W|s) P(s)}{P(W)} \\
          &= P(w_1 , w_2 , \cdots , w_n |s) P(s) \\
          &= P(w_1 |s)P(w_2 |s) \cdots P(w_n |s)
\end{aligned}
$$

# SMS 단문문자 스팸 분류 [^nb-spam-classifier]

[^nb-spam-classifier]: [Jesus M. Castagnetto (2015-01-03), SPAM/HAM SMS classification using caret and Naive Bayes](https://rpubs.com/jesuscastagnetto/caret-naive-bayes-spam-ham-sms)

나이브 베이즈를 활용해서 SMS 단문문자 스팸 분류기 모형을 다음과 같이 개발할 수 있다.
가장 먼저, 데이터는 국내에도 SMS 단문문자 스팸데이터가 있다면 공유되면 좋을텐데 아쉽게도 해외 
Tiago A. Almeida, José Maria Gómez Hidalgo 분들이 생성한 `SMS Spam Collection v.1` 데이터를 활용한다.


## 환경설정과 데이터 가져오기 

[http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip) 
웹사이트에서 직접 다운로드 해서 모형 개발 디렉토리 넣고 압축을 풀어 분석을 수행해도 된다.

``` {r naive-bayes-spam-import} 
# 0. 환경설정 -------------------------------------
library(tidyverse)
library(tidytext)
library(caret)

sms_raw <- read_delim("data/SMSSpamCollection/SMSSpamCollection.txt", delim="\t", col_names = FALSE)
```

## `tidytext` 데이터 전처리 {#naive-bayes-preprocessing}

기본적으로 스팸이냐 아니냐는 `sms_raw$type` 변수에 저장되어 있다.
즉, 스팸이냐 아니냐는 SMS 단문 메시지에 담긴 문자내용이 핵심인데, 
나이브 베이즈 모형에 넣도록 단어를 추출해서 이를 문선단어행렬(DocumentTermMatrix)로 변환시키는데 `tidytext` 팩키지를 사용해서 데이터를 정제하고, 기계학습 모형 적합을 위한 훈련/시험 데이터로 분할시킨다.

이 전체 과정을 `tidytext` 작업흐름에 맞춰 코드를 작성한다.

1. `mutate()` 함수로 스팸이냐 아니냐를 요인형으로 변환시킨다.
1. 단문 SMS 텍스트를 소문자로 `str_to_lower()` 함수로 변환시킨다.
1. `unnest_tokens()` 함수로 토큰화한다.
1. 불용어(`stop_words`) 사전을 사용해서 불용어를 `anti_join()` 함수로 제거한다.
1. 영단어를 위해서 `SnowballC` 팩키지 `wordStem()` 함수로 어간을 추출한다.
 
``` {r naive-bayes-spam-preprocessing}
# 2. 데이터 전처리 -------------------------------------
sms_tbl <- sms_raw %>% 
  as_tibble() %>% 
  set_names(c("type", "text")) %>% 
    mutate(type = ifelse(type=="spam", 1, 0) %>% as.factor) %>% 
    mutate(sms_id = row_number())

sms_token <- sms_tbl %>% 
  mutate(text = str_to_lower(text)) %>% 
  unnest_tokens(input=text, output="word", token="words") %>% 
  anti_join(stop_words) %>% 
  mutate(word = SnowballC::wordStem(word))
```

## `tidytext` DTM 생성 [^tidy-cast] {#naive-bayes-dtm}

[^tidy-cast]: [Julia Silge and David Robinson (2019-07-27), "Converting to and from Document-Term Matrix and Corpus objects"](https://cran.r-project.org/web/packages/tidytext/vignettes/tidying_casting.html)

`caret` 예측모형 생성을 위해서 `basetable`을 만들어야 한다.
이를 위해서 DTM(Document Term Matrix)를 생성한다. 이를 위해서 `cast_dtm()` 함수를 사용한다.

``` {r naive-bayes-spam-dtm}
sms_dtm <- sms_token %>% 
  count(sms_id, word) %>% 
  cast_dtm(document=sms_id, term=word,
           value =n, weighting = tm::weightTfIdf)
sms_dtm
```

예측모형을 만들 때 `DTM` 크기가 중요하다. 이를 위해서 `removeSparseTerms()` 함수를 사용해서 파일 크기를 줄이는 방법을 살펴보자.

``` {r naive-bayes-spam-dtm-size}
sparity_rates <- seq(0.9, 0.999, 0.01)
dtm_size <- list()

for(i in seq_along(sparity_rates)) {
  dtm_size[[i]] <- tm::removeSparseTerms(sms_dtm, sparse = sparity_rates[i])
  # cat(length(dtm_size[[i]]$dimnames$Terms), "\n")
  # cat(dtm_size[[i]]$dimnames$Docs, "\n")
}

sms_sparse_dtm <- tm::removeSparseTerms(sms_dtm, sparse = 0.99)
sms_sparse_dtm
```

# 스팸분류 예측모형 [^julia-supervised] [^social-science-text] {#naive-bayes-dtm-classification}

[^julia-supervised]: [(RE)LAUNCHING MY SUPERVISED MACHINE LEARNING COURSE](https://juliasilge.com/blog/supervised-ml-course/)

[^social-science-text]: [Supervised classification with text data](https://cfss.uchicago.edu/notes/supervised-text-classification/)

`DocumentTermMatrix` 객체를 7:3으로 훈련/시험 데이터로 나누고 `randomForest` 팩키지를 사용해서 "스팸/햄" 을 분류하는 스팸 분류예측모형을 텍스트 Feature만을 대상으로 개발한다.

``` {r naive-bayes-spam-classification}
library(randomForest)

# 훈련/시험 구분
sample_size <- floor(0.7 * nrow(sms_sparse_dtm))
train_ind <- sample(nrow(sms_sparse_dtm), size = sample_size)

sms_train <- sms_sparse_dtm[train_ind, ]
sms_test <- sms_sparse_dtm[-train_ind, ]

# Random Forest 예측모형
sms_rfc <- randomForest(x = as.data.frame(as.matrix(sms_train)), 
                    y = sms_tbl$type[train_ind])

sms_rfc
```


